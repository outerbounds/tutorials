{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06ecaed-2962-4051-a2d1-a0b1b622fbfa",
   "metadata": {},
   "source": [
    "This tutorial references this [notebook](https://github.com/outerbounds/tutorials/blob/main/cv/cv-intro-5.ipynb). The notebook shows how to analyze the results of your flow runs from the previous episodes. You will see how to fetch data from flow runs and interpret it with tags. This is an important aspect of the experience of working with Metaflow. You will see how to move between scripts and notebooks. In this case, you will use the Metaflow client API to tag promising runs as production candidates.\n",
    "\n",
    "After following the setup instructions, start the notebook with this command:\n",
    "```bash\n",
    "jupyter lab cv-intro-5.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a0721b-4d44-4e5c-83c1-56600bbfdec3",
   "metadata": {},
   "source": [
    "### Load Flow Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2912ab-6d72-428e-8c4f-a0d93e8cadb1",
   "metadata": {},
   "source": [
    "[Tagging](https://docs.metaflow.org/scaling/tagging#tagging) helps you organize flows. Tags let you apply interpretations to the results of flows. Let's see how they work by loading run data from the `TuningFlow` you built in [episode 4](/docs/cv-tutorial-S1E4). The data can be accessed in any Python environment using Metaflow's Client API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db3da181-4c0e-4d64-9385-66e80d7db4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Flow\n",
    "model_comparison_flow = Flow('ModelComparisonFlow')\n",
    "tuning_flow = Flow('TuningFlow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8524f-8abf-41fe-9bd6-fe505909f8a0",
   "metadata": {},
   "source": [
    "### Define How to Aggregate and Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd435a-34d8-4233-96ce-3d4cd9c14bd2",
   "metadata": {},
   "source": [
    "Next we define a function to parse the data in the runs. \n",
    "The customizable `get_stats` function will progressively build up a dictionary called `stats`.\n",
    "Each new entry in the `stats` dictionary contains hyperparameters, metrics, and metadata corresponding to a model trained in a `TuningFlow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a85a720-1f1a-422d-8f91-ef4749825cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def get_stats(stats, run, metrics):\n",
    "    if run.successful and hasattr(run.data, 'results'):\n",
    "        results = run.data.results\n",
    "        if not np.all(_m in results.columns for _m in metrics):\n",
    "            return stats\n",
    "        best_run = results.iloc[results[metrics[0]].idxmax()]\n",
    "        stats['flow id'].append(run.id)\n",
    "        stats['flow name'].append(run.parent.pathspec)\n",
    "        stats['model name'].append(best_run['model'])\n",
    "        for _m in metrics:\n",
    "            stats[_m].append(best_run[_m])\n",
    "        stats['test loss'].append(best_run['test loss'])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9063e8-6e36-41be-b73f-6e0ded7cc868",
   "metadata": {},
   "source": [
    "Next we loop through runs of `TuningFlow` and `ModelComparisonFlow` and aggregate `stats`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd75d86a-e1de-4f25-88da-0c896a725bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tuning_flow.latest_run.data.metrics\n",
    "\n",
    "stats = {\n",
    "    'flow id': [],\n",
    "    'flow name': [],\n",
    "    'model name': [],\n",
    "    'test loss': [],\n",
    "    **{metric: [] for metric in metrics}\n",
    "}\n",
    "\n",
    "for run in tuning_flow.runs():\n",
    "    stats = get_stats(stats, run, metrics)\n",
    "    \n",
    "for run in model_comparison_flow.runs():\n",
    "    stats = get_stats(stats, run, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71d2386b-5af0-4916-9ebc-323660cf1c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow id</th>\n",
       "      <th>flow name</th>\n",
       "      <th>model name</th>\n",
       "      <th>test loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision at recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1666721523161525</td>\n",
       "      <td>TuningFlow</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.026965</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.999272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1665967558891569</td>\n",
       "      <td>TuningFlow</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.027228</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.999168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1666721393687341</td>\n",
       "      <td>ModelComparisonFlow</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.026307</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.999272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1665967344088184</td>\n",
       "      <td>ModelComparisonFlow</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.030421</td>\n",
       "      <td>0.9892</td>\n",
       "      <td>0.998545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            flow id            flow name model name  test loss  accuracy  \\\n",
       "0  1666721523161525           TuningFlow        CNN   0.026965    0.9910   \n",
       "1  1665967558891569           TuningFlow        CNN   0.027228    0.9907   \n",
       "2  1666721393687341  ModelComparisonFlow        CNN   0.026307    0.9910   \n",
       "3  1665967344088184  ModelComparisonFlow        CNN   0.030421    0.9892   \n",
       "\n",
       "   precision at recall  \n",
       "0             0.999272  \n",
       "1             0.999168  \n",
       "2             0.999272  \n",
       "3             0.998545  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "best_models = pd.DataFrame(stats)\n",
    "best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd35be-a7b3-4044-9229-a18126a0cf63",
   "metadata": {},
   "source": [
    "### Access the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994b87e-bd07-487e-acc5-1b0b04a37b41",
   "metadata": {},
   "source": [
    "With the list of `best_models`, we can sort by `test accuracy` performance and find the run containing the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee1c1725-e1c5-4388-9438-4ee54131ff3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Run('TuningFlow/1666721523161525')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metaflow import Run\n",
    "sorted_models = best_models.sort_values(by=metrics[0], ascending=False).iloc[0]\n",
    "run = Run(\"{}/{}\".format(sorted_models['flow name'], sorted_models['flow id']))\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca9401-9f2e-49b5-8701-ff45a5381e99",
   "metadata": {},
   "source": [
    "Next, the model can be used to make predictions that we can check make sense when compared with the true targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92ba1b47-c141-44f8-a06f-02878e76f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51/313 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 13:25:12.526043: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# get data samples\n",
    "((x_train, y_train), (x_test, y_test)) = keras.datasets.mnist.load_data()\n",
    "x_test = np.expand_dims(x_test.astype(\"float32\") / 255, -1)\n",
    "\n",
    "# use best_model from the Metaflow run\n",
    "logits = run.data.best_model.predict(x_test)\n",
    "softmax = keras.layers.Softmax(axis=1)\n",
    "probs = softmax(logits).numpy()\n",
    "pred = probs.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b5ae773-d539-4974-99cf-957de4aa06a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicts [7 2 1 ... 4 5 6]\n",
      "  True targets [7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model predicts {}\".format(pred))\n",
    "print(\"  True targets {}\".format(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cbb75-ab90-41c0-bba6-169fa143cca6",
   "metadata": {},
   "source": [
    "### Interpret Results with Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf1733-1d17-4fa5-b133-e5792977523b",
   "metadata": {},
   "source": [
    "In the last section, you saw how to access and use the best model by filtering Metaflow runs.\n",
    "What if you want to add a property to runs so you can filter by that property later? \n",
    "Then it is time to leverage tagging. \n",
    "You can use `.add_tag` on runs that meet any condition.\n",
    "\n",
    "In this case, we consider models that have a `test accuracy > threshold`. \n",
    "Runs that have models meeting this threshold are tagged as `production`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87b9a59f-c9c4-4a7e-8993-008e78cc9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_runs(flow, metric = 'accuracy', threshold = 0.99):\n",
    "    for run in flow:\n",
    "        if run.successful and hasattr(run.data, 'results'):\n",
    "            if run.data.results[metric].max() > threshold:\n",
    "                run.add_tag('production')\n",
    "\n",
    "tag_runs(tuning_flow)\n",
    "tag_runs(model_comparison_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a52d9-636d-49bf-a9a0-e43649ed9ffe",
   "metadata": {},
   "source": [
    "Now runs can be accessed by filtering on this tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8068c2a8-fb39-46fc-80d2-ccd4f81ffa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Flow\n",
    "production_runs = Flow('TuningFlow').runs('production')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2595ad-d520-4065-bf4d-6e9a906aa54f",
   "metadata": {},
   "source": [
    "In this lesson, you saw how to load and analyze results of your flows. \n",
    "You added tags to runs that met your requirements for production quality.\n",
    "In the next lesson, you will see how to use models, filtered by the `production` tag, in a prediction flow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
