{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38a0296-1d6a-4c30-bbb9-8811367b71e1",
   "metadata": {},
   "source": [
    "Now that you have the core elements for a robust computer vision training environment, how can you use the `TrainHandGestureClassifier` flow to iteratively find the best models for your use case?\n",
    "\n",
    "For the rest of this tutorial, we will demonstrate two important elements of iterative model development: checkpointing model state and tracking experiment results.\n",
    "\n",
    "To follow along with this page, you can access this [Jupyter notebook](https://github.com/outerbounds/tutorials/tree/main/cv-2/cv-S2E5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fef15-2ebd-4437-932d-918d936864fd",
   "metadata": {},
   "source": [
    "### Checkpoint Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da2a6a-7535-4ff5-8d92-6116f9abdb4e",
   "metadata": {},
   "source": [
    "Checkpointing in model development essentially means that you save the state of a model, so you can resume it at a later time. This way you can make sure you do not lose results, such as your trained model. It also ensures you have a process to load an already trained model in future training and production scenarios while avoiding duplication of costly computation.\n",
    "\n",
    "In the PyTorch example used in the `TrainHandGestureClassifier` flow, a \"checkpoint\" refers to this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b77c15-d859-4438-8c0b-37e6349554ab",
   "metadata": {},
   "source": [
    "```python\n",
    "checkpoint_dict = {\n",
    "    'state_dict': model.cpu().state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'config': config_dict\n",
    "}\n",
    "torch.save(checkpoint_dict, checkpoint_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58606a50-e99e-40e6-9d2e-0c67ccecdc2d",
   "metadata": {},
   "source": [
    "You can save any artifact of the training work you have done in a `checkpoint_dict` like this.\n",
    "Then, you can resume the model state from the checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437435b3-791b-4ccd-99fc-39c64cafb5fb",
   "metadata": {},
   "source": [
    "```python\n",
    "from models.mobilenetv3 import MobileNetV3\n",
    "model = MobileNetV3(num_classes=num_classes, size='small', pretrained=pretrained, freezed=freezed)\n",
    "...\n",
    "checkpoint = torch.load(checkpoint, map_location=torch.device(device))[\"state_dict\"]\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "...\n",
    "model.to(device)\n",
    "return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a97b5-536f-4b37-aa46-37f8aa4d4550",
   "metadata": {},
   "source": [
    "[Here](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training) are more general resources from the PyTorch documentation on checkpointing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b1ae9-6842-4197-ab02-fb137b00d3d5",
   "metadata": {},
   "source": [
    "### Uploading the Best Model to the Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f8214-ca17-4d0e-942d-e6169bf1cf53",
   "metadata": {},
   "source": [
    "Model checkpoints in this example are written to the `best_model.pth` location. \n",
    "But if we are running on a remote compute instance, how do we move this checkpoint to a cloud resource that will persist beyond the lifecycle of the compute task? Again, Metaflow's S3 client makes this easy!\n",
    "\n",
    ":::note\n",
    "There are many ways to structure model development workflows. You may not have to store model checkpoints in the cloud, for example. You also might prefer to use Metaflow's [IncludeFile pattern](/docs/load-local-data-with-include/) to move data of this type onto remote compute instances. \n",
    ":::\n",
    "\n",
    "After each time model performs better than the previous best one, it is checkpointed and the result is uploaded to the cloud using this snippet:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eada043-699b-4895-bbd4-44ddf2d4bbea",
   "metadata": {},
   "source": [
    "```python\n",
    "path_to_best_model = os.path.join(experiment_path, 'best_model.pth')\n",
    "with S3(s3root = experiment_cloud_storage_path) as s3:\n",
    "    s3.put_files([(path_to_best_model, path_to_best_model)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be6b100-0d7d-410c-bd15-61f1927745cc",
   "metadata": {},
   "source": [
    "### Resuming the Best Model State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b428b-2f80-43d5-bf49-92d4fd8663a1",
   "metadata": {},
   "source": [
    "The payoff of checkpointing in this way is that now you can easily resume the model from this state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab4137-9549-40f4-91b6-8076136981c5",
   "metadata": {},
   "source": [
    "In a notebook or Python script you can now evaluate the model, train it further, or iterate on the model architecture (PyTorch allows you to build [dynamic graphs](https://cs230.stanford.edu/section/5/)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab3d890-0e4c-46a2-9581-e1cf54818e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ResNet18\n",
      "Building model from local checkpoint at: best_model.pth\n"
     ]
    }
   ],
   "source": [
    "from hagrid.classifier.run import _initialize_model\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "model_path = 'best_model.pth'\n",
    "\n",
    "try:\n",
    "    model = _initialize_model(\n",
    "        conf = OmegaConf.load('hagrid/classifier/config/default.yaml'),\n",
    "        model_name = 'ResNet18', \n",
    "        checkpoint_path = model_path, # can be local or S3 URI. \n",
    "        device = 'cpu'\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(\"Are you sure you trained a model and saved the file to {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9bbeb-74ed-4887-85f3-86b02e649f6d",
   "metadata": {},
   "source": [
    "Because of how the `TrainHandGestureClassifier` uses Metaflow's built-in versioning capabilities, we can also resume model training with the `--checkpoint` parameter when you run the `TrainHandGestureClassifier` defined in `classifier_flow.py`. \n",
    "\n",
    "![](../../../../static/assets/cv-tutorial-2-IteratingTrainHandGestureClassifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb5a60b-5bf2-4454-831e-570aabc30949",
   "metadata": {},
   "source": [
    "This checkpoint parameter can either be a `.pth` file in an S3 bucket or a path to a local `.pth` file. In general, you can add arbitrary parameter types like this to your Metaflow flows in one line of code. This helps you write flexible code that can read in data from external systems in a variety of ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7374a02-fc48-4b96-a42c-de8acb2bd930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.14\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mTrainHandGestureClassifier\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:eddie\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-12-04 01:04:46.544 \u001b[0m\u001b[1mWorkflow starting (run-id 187938):\u001b[0m\n",
      "\u001b[35m2022-12-04 01:04:48.222 \u001b[0m\u001b[32m[187938/start/1013688 (pid 28793)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-12-04 01:04:50.630 \u001b[0m\u001b[32m[187938/start/1013688 (pid 28793)] \u001b[0m\u001b[22mTraining ResNet18 in flow TrainHandGestureClassifier\u001b[0m\n",
      "\u001b[35m2022-12-04 01:04:54.178 \u001b[0m\u001b[32m[187938/start/1013688 (pid 28793)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-12-04 01:04:55.663 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-12-04 01:04:59.537 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22mDownloading images...\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:39.038 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22mDone!\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:39.986 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22mDownloading annotations...\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:39.986 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22mDone!\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:40.450 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:101] INFO     [2022-12-04 01:05:40,450]  Current device: mps\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:40.452 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22mBuilding ResNet18\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:43.696 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22mBuilding model from local checkpoint at: best_model.pth\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:44.932 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:230] INFO     [2022-12-04 01:05:44,931]  Epoch: 0\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:57.179 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:05:57,178]  Step 0: Loss = 3.666110038757324\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:57.546 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:05:57,544]  Step 1: Loss = 3.7233152389526367\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:57.883 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:05:57,883]  Step 2: Loss = 3.793562650680542\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:58.334 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:05:58,334]  Step 3: Loss = 3.6563098430633545\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:58.808 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:05:58,808]  Step 4: Loss = 3.651752233505249\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:59.261 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:05:59,261]  Step 5: Loss = 3.7605068683624268\u001b[0m\n",
      "\u001b[35m2022-12-04 01:05:59.704 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:05:59,704]  Step 6: Loss = 3.737733840942383\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:00.136 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:00,136]  Step 7: Loss = 3.643099308013916\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:00.574 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:00,574]  Step 8: Loss = 3.66499662399292\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:01.008 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:01,008]  Step 9: Loss = 3.5664844512939453\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:01.437 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:01,437]  Step 10: Loss = 3.556185483932495\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:01.869 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:01,869]  Step 11: Loss = 3.583991765975952\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:02.289 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:02,289]  Step 12: Loss = 3.6735057830810547\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:02.709 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:02,709]  Step 13: Loss = 3.493070363998413\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:03.155 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:03,155]  Step 14: Loss = 3.796480655670166\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:03.576 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:03,576]  Step 15: Loss = 3.5949411392211914\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:03.998 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:03,997]  Step 16: Loss = 3.5622711181640625\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:04.435 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:04,435]  Step 17: Loss = 3.65254282951355\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:04.869 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:04,869]  Step 18: Loss = 3.5879578590393066\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:05.297 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:05,297]  Step 19: Loss = 3.6369359493255615\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:05.726 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:05,725]  Step 20: Loss = 3.49463152885437\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:06.147 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:06,147]  Step 21: Loss = 3.456770420074463\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:07.116 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:155] INFO     [2022-12-04 01:06:07,116]  Step 22: Loss = 3.408698797225952\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:18.181 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:35] INFO     [2022-12-04 01:06:18,180]  valid: metrics for gesture\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:18.314 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:36] INFO     [2022-12-04 01:06:18,180]  {'accuracy': 0.168937, 'f1_score': 0.088073, 'precision': 0.063568, 'recall': 0.168937}\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:18.314 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:35] INFO     [2022-12-04 01:06:18,314]  valid: metrics for leading_hand\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:18.314 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:36] INFO     [2022-12-04 01:06:18,314]  {'accuracy': 0.577657, 'f1_score': 0.423016, 'precision': 0.333687, 'recall': 0.577657}\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:18.314 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m[LINE:251] INFO     [2022-12-04 01:06:18,314]  Saving best model with metric: 0.088073\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:51.571 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22mBest model checkpoint saved at s3://oleg2-s3-mztdpcvj/metaflow/TrainHandGestureClassifier/187938/experiments/ResNet18/best_model.pth\u001b[0m\n",
      "\u001b[35m2022-12-04 01:06:54.455 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[35m2022-12-04 01:08:37.606 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22mView Tensorboard results in your browser with this command:\u001b[0m\n",
      "\u001b[35m2022-12-04 01:08:37.606 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22mtensorboard --logdir=s3://oleg2-s3-mztdpcvj/metaflow/TrainHandGestureClassifier/187938/experiments/ResNet18/logs\u001b[0m\n",
      "\u001b[35m2022-12-04 01:08:37.606 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[35m2022-12-04 01:08:37.606 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[22mBest model checkpoint saved at s3://oleg2-s3-mztdpcvj/metaflow/TrainHandGestureClassifier/187938/experiments/ResNet18/best_model.pth\u001b[0m\n",
      "\u001b[35m2022-12-04 01:08:44.232 \u001b[0m\u001b[32m[187938/train/1013689 (pid 28800)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-12-04 01:08:45.804 \u001b[0m\u001b[32m[187938/end/1013690 (pid 28927)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-12-04 01:08:50.543 \u001b[0m\u001b[32m[187938/end/1013690 (pid 28927)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-12-04 01:08:50.837 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python classifier_flow.py --package-suffixes '.yaml' run --epochs 1 --model 'ResNet18' --checkpoint 'best_model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce544d4a-9da8-428c-861d-6ed3c3ccb4b5",
   "metadata": {},
   "source": [
    "In this lesson, you saw how to ensure you don't lose progress as you iterate on your model using checkpoints. \n",
    "You learned how to store model checkpoints and resume that state from a notebook or as the starting point in a subsequent flow. \n",
    "In the next lesson, we will complete the tutorial by demonstrating the use of [TensorBoard](https://www.tensorflow.org/tensorboard)'s experiment tracking solution with Metaflow. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
