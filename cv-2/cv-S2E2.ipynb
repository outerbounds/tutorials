{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a729e5c7-c8be-452e-8603-6910e082680e",
   "metadata": {},
   "source": [
    "### Why use a Torch DataLoader?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4fede7-e735-4bed-ac51-4e5b989bfc8c",
   "metadata": {},
   "source": [
    "PyTorch's built-in `Dataset` and `Dataloader` objects simplify the processes between ingesting data and feeding it to a model.\n",
    "The objects provide abstractions that address requirements common to most, if not all, deep learning scenarios. \n",
    "* The `Dataset` defines the structure and how to fetch data instances. \n",
    "* The `Dataloader` leverages the `Dataset` to load batches of data that can easily be shuffled, sampled, transformed, etc.\n",
    "\n",
    "Importantly for many computer vision cases, this PyTorch functionality is built to scale to training large networks on large datasets and there are many optimization avenues to explore for advanced users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0122e28d-f990-4f19-8b7e-2d63c741b7fd",
   "metadata": {},
   "source": [
    "### What is a Torch DataLoader?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6f201-8ef0-4b0d-b43a-6d24f2cf16f1",
   "metadata": {},
   "source": [
    "The [torch.utils.data.Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class helps you efficiently access batches from a dataset so you can feed them into your model.\n",
    "The `DataLoader` constructor has this signature:\n",
    "\n",
    "```\n",
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None, *, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "```\n",
    "You can read more detail [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). \n",
    "The most important argument is the `dataset`, which should be an instance of a `torch.utils.data.DataLoader` object. \n",
    "This object is what we will customize next.\n",
    "Then we can use it to instantiate `Dataloader` objects that follow the standard pattern for feeding data into a PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8a999-dc1f-4cb6-8e21-9af14a05b262",
   "metadata": {},
   "source": [
    "### Build a Torch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9684c556-3a4a-49fe-90ff-2988a6507c0c",
   "metadata": {},
   "source": [
    "To create a `Dataloader`, we need to pass it a `Dataset`.  \n",
    "There are two ways to define a Torch `Dataset` object, the map and the iterable style. \n",
    "The difference is whether the `torch.utils.data.Dataset` class defines the `__len__` and `__getitem__` functions (map type) or the `__iter__` function (iterable type). \n",
    "You can read more about this distinction [here](https://pytorch.org/docs/stable/data.html#dataset-types).\n",
    "For now, all you need to know in the rest of this episode you will build a custom dataset with the HaGRID data `GestureDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15047c85-d5e1-44b2-8a64-6b8bcf80d88a",
   "metadata": {},
   "source": [
    "### Example: Components of the GestureDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd795f55-c691-41c5-b648-42cf3a064112",
   "metadata": {},
   "source": [
    "In all remaining notebook examples and flows in this tutorial, we will use the `GestureDataset`. \n",
    "Much of the code is reused from the original source, which you can view [here](https://github.com/hukenovs/hagrid/blob/master/classifier/dataset.py).\n",
    "The end goal is to create a `GestureDataset` object that we can easily use in model training code like the following snippet:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d2c3f1-4f87-4aae-96d1-bd5b96e79990",
   "metadata": {},
   "source": [
    "```python\n",
    "model = _initialize_model(model_name, checkpoint_path, device)\n",
    "train_dataset = GestureDataset(is_train=True, transform=get_transform())\n",
    "test_dataset = GestureDataset(is_train=False, transform=get_transform())\n",
    "TrainClassifier.train(model, train_dataset, test_dataset, device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9615c4e0-db2e-47fa-b714-6b6aeb40ca13",
   "metadata": {},
   "source": [
    "This section shows how to implement the methods needed to use `GestureDataset`, or any custom dataset, as depicted in the above code. \n",
    "More than the details of this specific example code, the main takeaway of this section is that when working with a custom `Dataset` class you need to:\n",
    "1. Your class should be a subclass of `torch.utils.data.Dataset`.\n",
    "2. You need to define the constructor.\n",
    "3. You either need to define the `__getitem__` and `__len__` methods, or define the `__iter__` method. You can put whatever you want in the different methods of your `Dataset` classes so long as the function signatures follow the [PyTorch protocol](https://pytorch.org/docs/stable/data.html#dataset-types)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae2a2b-78a1-49e2-81aa-24eecf76e75f",
   "metadata": {},
   "source": [
    "#### The Dataset Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb951a37-d536-45a7-985a-625051f086e3",
   "metadata": {},
   "source": [
    "The `Dataset` constructor is called upon to create the dataset. \n",
    "For `GestureDataset`, the constructor does the following:\n",
    "* Assign class variables for a configuration file, transformations, and dataset labels. \n",
    "* Split the images and their annotations into training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ece6fd-cbce-4e8e-b7a6-79020cebe272",
   "metadata": {},
   "source": [
    "```python\n",
    "class GestureDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, is_train, conf, transform = None, is_test = False):\n",
    "        self.conf = conf\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.labels = {\n",
    "            label: num for (label, num) in zip(self.conf.dataset.targets, range(len(self.conf.dataset.targets)))\n",
    "        }\n",
    "        self.leading_hand = {\"right\": 0, \"left\": 1}\n",
    "        subset = self.conf.dataset.get(\"subset\", None)\n",
    "        self.annotations = self.__read_annotations(subset)\n",
    "        users = self.annotations[\"user_id\"].unique()\n",
    "        users = sorted(users)\n",
    "        random.Random(self.conf.random_state).shuffle(users)\n",
    "        train_users = users[: int(len(users) * 0.8)]\n",
    "        val_users = users[int(len(users) * 0.8) :]\n",
    "        self.annotations = self.annotations.copy()\n",
    "        if not is_test:\n",
    "            if is_train:\n",
    "                self.annotations = self.annotations[self.annotations[\"user_id\"].isin(train_users)]\n",
    "            else:\n",
    "                self.annotations = self.annotations[self.annotations[\"user_id\"].isin(val_users)]\n",
    "                \n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcbdd79-5e2b-4143-be3d-65c86fe7ae49",
   "metadata": {},
   "source": [
    "#### Getting a Data Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce3310-85ed-42bf-9598-df7d6f23651b",
   "metadata": {},
   "source": [
    "The `__getitem__` is a class method that allows instances of the `Dataset` class to be indexed like a list using `[]`. \n",
    "In our case, we want this function to take an integer `index` and return an appropriately sized image and its label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3de781-e05f-47e3-b9c3-b4441b42d43f",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class GestureDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        row = self.annotations.iloc[[index]].to_dict(\"records\")[0]\n",
    "        image_resized, gesture, leading_hand = self.__prepare_image_target(\n",
    "            row[\"target\"], row[\"name\"], row[\"bboxes\"], row[\"labels\"], row[\"leading_hand\"]\n",
    "        )\n",
    "        label = {\"gesture\": self.labels[gesture], \"leading_hand\": self.leading_hand[leading_hand]}\n",
    "        if self.transform is not None:\n",
    "            image_resized, label = self.transform(image_resized, label)\n",
    "        return image_resized, label\n",
    "    \n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c313f8-a5e7-4027-8b1f-9883c68cba92",
   "metadata": {},
   "source": [
    "### Example: Using the GestureDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca99a172-3693-456b-a8d6-d0599fb2d963",
   "metadata": {},
   "source": [
    "In this section, you will use the `GestureDataset` to instantiate a `Dataloader` and visualize one batch of images with their labels.\n",
    "\n",
    "First, we will import dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf1a2d7-9e90-4d3a-a511-46721ccd28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hagrid.classifier.dataset import GestureDataset\n",
    "from hagrid.classifier.preprocess import get_transform\n",
    "from hagrid.classifier.utils import collate_fn\n",
    "from omegaconf import OmegaConf\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path_to_config = './hagrid/classifier/config/default.yaml'\n",
    "conf = OmegaConf.load(path_to_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c001b0bb-193e-45b4-8055-dfe0a98a0444",
   "metadata": {},
   "source": [
    "Then we instantiate the `GestureDataset` implemented [here](https://github.com/outerbounds/tutorials/cv-2/hagrid/classifier/dataset.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8329e012-fc12-44a0-bba0-5c809b62b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GestureDataset(is_train=True, conf=conf, transform=get_transform())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34266b-e24e-4644-8a5f-b8eba1b3cbd3",
   "metadata": {},
   "source": [
    "Now, you can use the `train_dataset` to create a data loader to request batches from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528cd76e-5577-425c-95fd-8a86a2c241a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=1, # change this to load data faster. feasible values depend on your machine specs. \n",
    "    collate_fn=collate_fn, \n",
    "    shuffle=True \n",
    "    # What happens \n",
    "    # to the image grid displayed by the view_batch function\n",
    "    # when you set shuffle=False in this constructor?\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e47e4-3186-40d7-b8a6-4b2e0903ee8c",
   "metadata": {},
   "source": [
    "Here is a helper function to show the contents of a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97122ad2-623d-4111-a84a-d92ef8c29c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_batch(images, labels, batch_size):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.ioff()\n",
    "    grid_dim = (\n",
    "        int(sqrt(batch_size)), \n",
    "        int(sqrt(batch_size)) + (1 if sqrt(batch_size) % 1 > 0 else 0)\n",
    "    )\n",
    "    fig, axes = plt.subplots(*grid_dim)\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        x, y = i//grid_dim[1], i%grid_dim[1]\n",
    "        image = image.permute(1,2,0)\n",
    "        axes[x, y].imshow(image) \n",
    "        axes[x, y].set_title(conf.dataset.targets[label['gesture']], fontsize=10)\n",
    "        [axes[x, y].spines[_dir].set_visible(False) for _dir in ['right', 'left', 'top', 'bottom']]\n",
    "        axes[x, y].set_xticks([])\n",
    "        axes[x, y].set_yticks([])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fname='./dataloader-sample.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba63f80-8996-4877-ab90-56c331843cf9",
   "metadata": {},
   "source": [
    "Now we can take the next batch from the `train_dataloader` and view a grid of each image and its corresponding label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c42cd5-bcb8-467d-b99c-d546614b54c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "view_batch(images, labels, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b36f52-f6da-4c7f-afe2-bc865c84d518",
   "metadata": {},
   "source": [
    "![](dataloader-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e5fd0-80b3-4b69-b4eb-a9b0aaf2b630",
   "metadata": {},
   "source": [
    "Nice! Getting a reliable data flow is a big step in any machine learning project.\n",
    "In this lesson, you have just scratched the surface of the tools PyTorch offers to help you do this. \n",
    "You learned about PyTorch datasets and data loaders in this episode.\n",
    "You saw to use them to efficiently and reliably load HaGRID dataset samples for training PyTorch models.\n",
    "Looking forward you will pair PyTorch data loaders with Metaflow features to extend the concepts when working with datasets in models in the cloud. \n",
    "See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
