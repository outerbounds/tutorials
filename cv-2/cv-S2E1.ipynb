{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d591063d-0f9b-48be-85bb-84263053ff8a",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a0b32-3f8b-4771-9f2a-784674ae89a3",
   "metadata": {},
   "source": [
    "In this tutorial, you will build an image classifier on a large image dataset. You will learn how to move large amounts of data between your local environment, S3 storage, and remote compute instances where models are trained. You will fine-tune state-of-the-art model architectures on cloud GPUs and track results with Tensorboard. Before diving into these details, let's meet the dataset we will use to guide the tutorial.\n",
    "\n",
    "This tutorial has six episodes. If you want to run the code, you can follow along with this first page in this [Jupyter notebook](https://github.com/outerbounds/tutorials/tree/main/cv-2/cv-S2E1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f206d-3675-42b2-b086-718dabf8955e",
   "metadata": {},
   "source": [
    "### What is the HaGRID Dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fa6d5-20f8-4a83-8e46-a34a2f51ef58",
   "metadata": {},
   "source": [
    "![](../../../../static/assets/hagrid.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b969c89-c897-411b-bb4a-3d7cd68400cc",
   "metadata": {},
   "source": [
    "HaGRID is a large image dataset with labels and annotations for classification or detection tasks.\n",
    "The full HaGRID dataset is 716GB with 552,992 images divided into [18 classes of hand gestures](https://github.com/hukenovs/hagrid#tranval). Conveniently, the authors provide an evenly split (by class) 2GB sample that leads to cloud runs you can complete in one sitting.\n",
    "You can find more details in the [GitHub repository](https://github.com/hukenovs/hagrid) and corresponding paper, [HaGRID - HAnd Gesture Recognition Image Dataset](https://arxiv.org/abs/2206.08219). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409979d-9e46-4de9-aacd-b88052db3843",
   "metadata": {},
   "source": [
    "### Download the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e898e7b-d1d1-475e-a5fd-d4c1abae2c41",
   "metadata": {},
   "source": [
    "You can use [wget](https://www.gnu.org/software/wget/) to download the subsample data from the URLs provided by the authors. The subsample will download 100 images from each class. Run the following from the command line to fetch the zipped data and place the zip file in the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf28121-8e4d-4f56-b796-2447fdb2cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-14 13:44:13--  https://sc.link/AO5l\n",
      "Resolving sc.link (sc.link)... 37.230.233.245\n",
      "Connecting to sc.link (sc.link)|37.230.233.245|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://n-usr-2uzac.s3pd02.sbercloud.ru/b-usr-2uzac-mv4/hagrid/subsample.zip [following]\n",
      "--2022-11-14 13:44:15--  https://n-usr-2uzac.s3pd02.sbercloud.ru/b-usr-2uzac-mv4/hagrid/subsample.zip\n",
      "Resolving n-usr-2uzac.s3pd02.sbercloud.ru (n-usr-2uzac.s3pd02.sbercloud.ru)... 37.18.122.129\n",
      "Connecting to n-usr-2uzac.s3pd02.sbercloud.ru (n-usr-2uzac.s3pd02.sbercloud.ru)|37.18.122.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2520202487 (2.3G) [application/zip]\n",
      "Saving to: ‘data/subsample.zip’\n",
      "\n",
      "data/subsample.zip  100%[===================>]   2.35G  3.79MB/s    in 14m 32s \n",
      "\n",
      "2022-11-14 13:58:50 (2.76 MB/s) - ‘data/subsample.zip’ saved [2520202487/2520202487]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! mkdir data && wget 'https://sc.link/AO5l' -O 'data/subsample.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b23909-9e63-48f2-949e-534c83c4ddbc",
   "metadata": {},
   "source": [
    "Then you can unzip the resulting `subsample.zip` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e0cf78-14b4-4119-8376-6638c89915e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -qq 'data/subsample.zip' -d 'data/subsample'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d19d4-aa68-433c-80c9-b67ae6096a30",
   "metadata": {},
   "source": [
    "### View Sample Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36296a54-c3e6-428c-946e-09ec3dea50db",
   "metadata": {},
   "source": [
    "Let's look at one class of images.\n",
    "You can see the available gesture labels by looking at the directories created when you unzipped the subsample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "850579c9-a556-4ad6-9065-2065b0590b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mcall\u001b[m\u001b[m            \u001b[34mlike\u001b[m\u001b[m            \u001b[34mpalm\u001b[m\u001b[m            \u001b[34mstop\u001b[m\u001b[m            \u001b[34mtwo_up\u001b[m\u001b[m\n",
      "\u001b[34mdislike\u001b[m\u001b[m         \u001b[34mmute\u001b[m\u001b[m            \u001b[34mpeace\u001b[m\u001b[m           \u001b[34mstop_inverted\u001b[m\u001b[m   \u001b[34mtwo_up_inverted\u001b[m\u001b[m\n",
      "\u001b[34mfist\u001b[m\u001b[m            \u001b[34mok\u001b[m\u001b[m              \u001b[34mpeace_inverted\u001b[m\u001b[m  \u001b[34mthree\u001b[m\u001b[m\n",
      "\u001b[34mfour\u001b[m\u001b[m            \u001b[34mone\u001b[m\u001b[m             \u001b[34mrock\u001b[m\u001b[m            \u001b[34mthree2\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls 'data/subsample'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f8da7-b863-4d90-bd72-77b43e885cab",
   "metadata": {},
   "source": [
    "In the next cell, pick a `gesture` variable from one of the [18 dataset labels](https://github.com/hukenovs/hagrid#tranval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81dabdea-a0e0-44ec-bcce-f279f500b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_data_path = 'data/subsample'\n",
    "gesture = 'peace'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56f9e0-05f8-46a3-9f91-e37ae98e5efe",
   "metadata": {},
   "source": [
    "Then we can grab sample images from the corresponding folder and visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "911d7469-d989-4f49-af3b-1a5bd1a3d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "N_IMAGES = 3\n",
    "AX_DIM = 3\n",
    "path = (os.getcwd(), relative_data_path, gesture, '*.jpg')\n",
    "sample_images = random.sample(glob.glob(os.path.join(*path)), N_IMAGES)\n",
    "plt.ioff()\n",
    "fig, axes = plt.subplots(\n",
    "    1, len(sample_images), \n",
    "    figsize = (AX_DIM * len(sample_images), AX_DIM)\n",
    ")\n",
    "fig.tight_layout()\n",
    "\n",
    "for img, ax in zip(sample_images, axes):\n",
    "\n",
    "    # configure axis\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # display image\n",
    "    ax.imshow(Image.open(img))\n",
    "\n",
    "fig.savefig(fname='{}-sample.png'.format(gesture));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65474908-1cc0-49ff-9776-7f3bfe2ca3b5",
   "metadata": {},
   "source": [
    "![](peace-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7afc1-0606-4d1a-889c-b6de991210e7",
   "metadata": {},
   "source": [
    "Similar to the command to download the images, you can download annotations using `wget`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3437bd8f-cda1-4cac-a72a-da230a6652c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-14 14:00:23--  https://sc.link/EQ5g\n",
      "Resolving sc.link (sc.link)... 37.230.233.245\n",
      "Connecting to sc.link (sc.link)|37.230.233.245|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://n-usr-2uzac.s3pd02.sbercloud.ru/b-usr-2uzac-mv4/hagrid/ann_subsample.zip [following]\n",
      "--2022-11-14 14:00:24--  https://n-usr-2uzac.s3pd02.sbercloud.ru/b-usr-2uzac-mv4/hagrid/ann_subsample.zip\n",
      "Resolving n-usr-2uzac.s3pd02.sbercloud.ru (n-usr-2uzac.s3pd02.sbercloud.ru)... 37.18.122.129\n",
      "Connecting to n-usr-2uzac.s3pd02.sbercloud.ru (n-usr-2uzac.s3pd02.sbercloud.ru)|37.18.122.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1297620 (1.2M) [application/zip]\n",
      "Saving to: ‘./data/subsample-annotations.zip’\n",
      "\n",
      "./data/subsample-an 100%[===================>]   1.24M  1.28MB/s    in 1.0s    \n",
      "\n",
      "2022-11-14 14:00:26 (1.28 MB/s) - ‘./data/subsample-annotations.zip’ saved [1297620/1297620]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget 'https://sc.link/EQ5g' -O 'data/subsample-annotations.zip'\n",
    "! unzip -qq 'data/subsample-annotations.zip'  -d 'data/subsample-annotations'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b18a8-c0cb-4830-84c2-d61c43a0eb9a",
   "metadata": {},
   "source": [
    "Let's inspect the annotations. \n",
    "The following code will draw a green box around the gesture of interest and a red box around other hands labeled in the image that are not making a gesture.\n",
    "These boxes correspond to the `bboxes` property that comes with each image annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "402bfb27-c77a-48f3-9e8f-6ef72e907f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "\n",
    "relative_annotation_path = 'data/subsample-annotations/ann_subsample/{}.json'.format(gesture)\n",
    "result = json.load(open(relative_annotation_path))\n",
    "\n",
    "color = None\n",
    "AX_DIM = 3\n",
    "plt.ioff()\n",
    "fig, axes = plt.subplots(1, len(sample_images), figsize = (AX_DIM * len(sample_images), AX_DIM))\n",
    "\n",
    "for im_file, ax in zip(sample_images, axes):\n",
    "\n",
    "    # get image\n",
    "    img_key = im_file.split('/')[-1].split('.')[0]\n",
    "    image = cv2.imread(im_file)\n",
    "    \n",
    "    # openCV dims are BGR \n",
    "    b,g,r = cv2.split(image)  \n",
    "    image = cv2.merge([r,g,b]) \n",
    "\n",
    "    # fetch bounding box for gesture\n",
    "    for i, label in enumerate(result[img_key]['labels']):\n",
    "\n",
    "        # determine annotation type\n",
    "        if label == gesture:\n",
    "            color = (0, 255, 0)\n",
    "        elif label == 'no_gesture':\n",
    "            color = (255, 0, 0)\n",
    "\n",
    "        # unpack annotation format\n",
    "        bbox = result[img_key]['bboxes'][i]\n",
    "        top_left_x, top_left_y, w, h = bbox\n",
    "        scale_x = image.shape[1]\n",
    "        scale_y = image.shape[0]\n",
    "\n",
    "        # draw bounding box to image scale\n",
    "        x1 = int(top_left_x * scale_x)\n",
    "        y1 = int(top_left_y * scale_y)\n",
    "        x2 = int(x1 + scale_x * w)\n",
    "        y2 = int(y1 + scale_y * h)\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness=3)\n",
    "        \n",
    "        # display image\n",
    "        ax.imshow(image)\n",
    "\n",
    "    # configure axis\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(img_key, fontsize=8)\n",
    "    \n",
    "fig.savefig('{}-sample-bbox.png'.format(gesture))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447890d9-695f-4f9d-93e6-03683a8b003e",
   "metadata": {},
   "source": [
    "![](peace-sample-bbox.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e11d4-6229-43d3-9a87-7019874afb01",
   "metadata": {},
   "source": [
    "### A Baseline Gesture Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5dc6ff-940e-4b5c-a5aa-cc2373421258",
   "metadata": {},
   "source": [
    "The learning task of interest in this tutorial is to classify images by gesture. \n",
    "In the previous section, you saw that each image comes with a `gesture` label and a bounding box in the corresponding annotation.\n",
    "Let's build a baseline model to predict the gesture for each image. \n",
    "We use the majority-class classifier, which measures what happens when we predict all of examples in the test set with the majority class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52d364-87e9-4703-beb6-88e50032bbd3",
   "metadata": {},
   "source": [
    "First, lets load the dataset using PyTorch objects you will learn about in the next episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "94a8bed5-81cb-49b4-bf36-4605fea1741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hagrid.classifier.dataset import GestureDataset\n",
    "from hagrid.classifier.preprocess import get_transform\n",
    "from hagrid.classifier.utils import collate_fn\n",
    "from omegaconf import OmegaConf\n",
    "from torch import nn, Tensor\n",
    "\n",
    "path_to_config = './hagrid/classifier/config/default.yaml'\n",
    "conf = OmegaConf.load(path_to_config)\n",
    "N_CLASSES = 19 \n",
    "\n",
    "test_dataset = GestureDataset(is_train=False, conf=conf, transform=get_transform())\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=conf.train_params.test_batch_size,\n",
    "    num_workers=conf.train_params.num_workers,\n",
    "    shuffle='random',\n",
    "    collate_fn=collate_fn,\n",
    "    persistent_workers = True,\n",
    "    prefetch_factor=conf.train_params.prefetch_factor,\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5422940-9169-4f17-b3ff-1314c86b5084",
   "metadata": {},
   "source": [
    "Then let's check the performance of the baseline model (always predict class 0) on one pass through the test set.\n",
    "\n",
    "Next, we collect the true targets next to compare to our benchmark approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0f1dcdc5-11e3-4954-96e1-5f912cd1e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "targets = defaultdict(list)\n",
    "n_targets_seen = defaultdict(int)\n",
    "for i, (images, labels) in enumerate(test_dataloader):\n",
    "    accuracies = {target:[] for target in list(labels)[0].keys()}\n",
    "    for target in list(labels)[0].keys():\n",
    "        target_labels = [label[target] for label in labels]\n",
    "        targets[target] += target_labels\n",
    "        n_targets_seen[target] += len(target_labels)\n",
    "        \n",
    "target = 'gesture'\n",
    "targets = torch.tensor(targets[target], dtype=torch.int32)\n",
    "predicts_labels = torch.zeros(n_targets_seen[target], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fcda14-8b44-4e55-968f-87c7ab959727",
   "metadata": {},
   "source": [
    "Finally, we compute metric scores that we will be tracking on data subsets that we evaluate at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7a69c31d-31bb-40b4-9eec-4c0a81a3b983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.05177111551165581,\n",
       " 'f1_score': 0.0050966376438736916,\n",
       " 'precision': 0.002680248348042369,\n",
       " 'recall': 0.05177111551165581}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.functional import accuracy, f1_score, precision, recall, auroc, confusion_matrix\n",
    "num_classes = 19\n",
    "average = conf.metric_params[\"average\"]\n",
    "metrics = conf.metric_params[\"metrics\"]\n",
    "scores = {\n",
    "    \"accuracy\": accuracy(predicts_labels, targets, average=average, num_classes=num_classes).item(),\n",
    "    \"f1_score\": f1_score(predicts_labels, targets, average=average, num_classes=num_classes).item(),\n",
    "    \"precision\": precision(predicts_labels, targets, average=average, num_classes=num_classes).item(),\n",
    "    \"recall\": recall(predicts_labels, targets, average=average, num_classes=num_classes).item()\n",
    "}\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94bc19-3bf5-4e1f-9250-872e15a076ed",
   "metadata": {},
   "source": [
    "In our baseline model, we see accuracy somewhere around 5% which makes sense given we have 18 evenly distributed classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad4ca50-ea36-40da-addc-e091e14031f2",
   "metadata": {},
   "source": [
    "In this episode, you were introduced to the HaGRID dataset. Each data point is labeled with a class from 18 different hand gesture labels. In the rest of this tutorial, you will learn how to build a computer vision model training workflow to predict hand gesture classes using this data. The next episode starts this journey by introducing the fundamentals of PyTorch data loaders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('3.10.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "60d98827d7482d2a0f6aae287a18990d3a1d423e0f66197ec6cdef8a2e07b41f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
